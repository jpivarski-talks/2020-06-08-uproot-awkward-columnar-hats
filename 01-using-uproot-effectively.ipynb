{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Uproot effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this works as a hands-on tutorial\n",
    "\n",
    "Even though I don't have formal exercises scattered throughout these notebooks, this session can still be interactive.\n",
    "\n",
    "   * **You** should open each notebook in Binder (see [GitHub README](https://github.com/jpivarski/2020-06-08-uproot-awkward-columnar-hats)) and evaluate cells, following along with me.\n",
    "   * **I** should pause frequently and stay open to questions. I'll be monitoring the videoconference chat.\n",
    "   * **We** should feel free to step off the path and try to answer \"What if?\" questions in real time.\n",
    "\n",
    "Not all digressions will lead to an answer—I often realize, \"That's why it didn't work!\" long after the tutorial is over—but tinkering is how we learn.\n",
    "\n",
    "Consider this a tour and I'm your guide. The planned route is a suggestion to get things started, but your questions and wayfaring are more important.\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasingly unnecessary introduction to/motivation for Python\n",
    "\n",
    "I used to start these tutorials by asking, \"Why Python?\" but that doesn't seem necessary anymore.\n",
    "\n",
    "![](img/python-usage.png)\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"15\">Introduction to </font><img src=\"img/uproot-logo-300px.png\" style=\"vertical-align:middle\">\n",
    "\n",
    "![](img/abstraction-layers.png)\n",
    "\n",
    "Uproot is an independent implementation of ROOT I/O and only I/O, using standard Python libraries wherever possible.\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why was it written?\n",
    "\n",
    "![](img/uproot-awkward-timeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uproot was originally a part of Femtocode, a query language for calculations on columnar data. I needed an easier way to deploy ROOT I/O.\n",
    "\n",
    "   * **Uproot 1.x** was released as a Python package \"in case anyone finds it useful.\"\n",
    "   * Machine learning users did find it useful, so I quickly cleaned it up and made it presentable as **Uproot 2.x**.\n",
    "   * The way people were using Uproot influenced how I thought about columnar analysis: breaking it out into smaller pieces and eventually the exposing array-at-a-time interface to users, rather than hiding the columnar processing behind a query language.\n",
    "   * Uproot's \"bottom up\" JaggedArrays were moved into a new package, **Awkward Array**, replacing the \"top down\" view of OAMap. This was **Uproot 3.x**.\n",
    "   * Awkward Array is successful even though it has interface flaws and its pure Python \"no for loops!\" implementation is hard to maintain.\n",
    "   * **Awkward 1.x** started last fall with a long development time to \"do it right.\" It is complete, but not very visible because Uproot doesn't produce the new-style arrays yet.\n",
    "   * **Uproot 4.x** started development in May with a release date of July 1.\n",
    "\n",
    "Unlike previous version updates (which were more minor), Uproot 3.x and Awkward 0.x will continue to exist as `uproot3` and `awkward0`.\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime this summer, `uproot4` → `uproot` and `awkward1` → `awkward`. If you need to keep old scripts working, you'll be able to\n",
    "\n",
    "```python\n",
    "import uproot3 as uproot\n",
    "import awkward0 as awkward\n",
    "```\n",
    "\n",
    "but new work should use the new libraries. (The old ones will continue to exist, though won't be actively maintained.)\n",
    "\n",
    "![](img/Raiders-of-the-Lost-Ark-Chamber.jpg)\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening a file with Uproot\n",
    "\n",
    "The read-only interface starts with `uproot.open`.\n",
    "\n",
    "(Also supports HTTP and XRootD URLs, but I don't cover them in this tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "file = uproot.open(\"data/nesteddirs.root\")\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file has a dict-like interface, meaning that you can access objects with square brackets and list them with `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"one\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.allkeys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.allclassnames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the `b` at the beginning of each file path?\n",
    "\n",
    "These are bytestrings, not strings, and Python 3 emphasizes the difference.\n",
    "\n",
    "I was worried that old ROOT files would use strange encodings and thought that presuming everything to be UTF-8 would make hist�gr�m title� l��k like th�s.\n",
    "\n",
    "But the issue of encodings never came up. Dealing with the Python bytestrings has been more of a nuisance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology preview: Uproot 4\n",
    "\n",
    "Uproot 4 is only half-written and might fail in simple cases. However, we can try it out side-by-side with Uproot 3 because it has a different package name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot4\n",
    "\n",
    "file_uproot4 = uproot4.open(\"data/nesteddirs.root\")\n",
    "file_uproot4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive=True is now the default; there's no 'allkeys'\n",
    "file_uproot4.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a dict, and no bytestrings\n",
    "file_uproot4.classnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4.classname_of(\"one/two/tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4.classname_of(\"one/two/tree;1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more bytestrings. (Invalid UTF-8 uses the \"surrogate escape\" method, so a strangely encoded string won't _break_ anything, at least.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the `;1` at the end of the key name?\n",
    "\n",
    "These are ROOT \"cycle numbers,\" which allow objects with the same name to exist in the same directory. We display them to disambiguate, but you don't have to type them to look up an object. (You'll get the latest one; the one with the highest cycle.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring a TTree\n",
    "\n",
    "TTrees also have a dict-like interface, though the `show` method has been very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = file[\"one/two/tree\"]\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left column: branch names, middle column: streamers (which define complex types), right column: how _we_ interpret the branch as an array (Uproot-specific)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"Float64\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"ArrayInt32\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"SliceInt64\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last of these is a jagged array, which has a variable number of items in each entry.\n",
    "\n",
    "   * Uproot 3 returns NumPy arrays for scalar and fixed-length per entry types.\n",
    "   * Uproot 3 returns Awkward 0 JaggedArrays for variable-length per entry types.\n",
    "   * Uproot 4 (by default) returns Awkward 1 arrays for all branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4[\"one/two/tree/Float64\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4[\"one/two/tree/ArrayInt32\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4[\"one/two/tree/SliceInt64\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still possible to get NumPy arrays with `library=\"np\"` (i.e. return type depends on what you ask for, not the contents of the file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4[\"one/two/tree/Float64\"].array(library=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4[\"one/two/tree/ArrayInt32\"].array(library=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4[\"one/two/tree/SliceInt64\"].array(library=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, Pandas is a `library`, rather than a special function, as well as CuPy (GPU arrays) and any others we might want to add in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_uproot4[\"one/two/tree/SliceInt64\"].array(library=\"pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ROOT data are organized\n",
    "\n",
    "Objects in directories are referenced by TKeys—you can ignore these, as they just make the square brackets syntax work.\n",
    "\n",
    "A TTree's TBranches are either containers of data, convertible to arrays, or placeholders in a hierarchy describing a \"split\" object (more on that later).\n",
    "\n",
    "The actual data are broken up into TBaskets, which is the smallest unit that can be read from a compressed file. There's no such thing as \"reading one event,\" unless you have one TBasket per event (which would be inefficient when reading many events).\n",
    "\n",
    "![](img/terminology.png)\n",
    "\n",
    "Often, you can ignore TBaskets: Uproot treats TBranches as the fundamental unit, with one TBranch → one array.\n",
    "\n",
    "But if your file compresses poorly or is slow to read, check the TBasket sizes to see that they are at least 10's to 100's of kilobytes each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = uproot.open(\"data/cms_opendata_2012_nanoaod_DoubleMuParked.root\")[\"Events\"]\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in events.keys():\n",
    "    print(f\"{name.decode():20} {events[name].numbaskets:2d} baskets {[events[name].basket_uncompressedbytes(i)/1024 for i in range(events[name].numbaskets)]} kB each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This affects ROOT performance, but it affects Uproot performance _more_.\n",
    "\n",
    "![](img/root-none-muon.png)\n",
    "\n",
    "(The TFile-TTree-TBranch-TBasket structure has to be navigated in slow Python, but reading/decompressing/interpreting a TBasket is a NumPy call, about as fast as the hardware allows.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split objects\n",
    "\n",
    "ROOT TTrees are intended to deliver collections of C++ objects. Strictly speaking, these objects have no equivalent in Python—certainly their C++ methods can't be executed by Python. (The C++ code is not stored in the file with the data, even if we had a runtime C++ compiler. That's why some ROOT scripts require `.L` to load libraries.)\n",
    "\n",
    "What the ROOT files _do_ provide is a list of each class's private member data and how they are laid out in bytes (called the `TStreamerInfo`). We can use that to generate Python classes and reconstruct the objects. However, that has to run in slow Python, not fast NumPy.\n",
    "\n",
    "As a storage optimization, ROOT files can be written with each member datum in a separate branch. This is called the \"splitLevel\" and [you can control it when writing files](https://root.cern.ch/doc/master/classTTree.html#addingacolumnofobjs) (if you have access to the process that writes files).\n",
    "\n",
    "Split data are\n",
    "\n",
    "   * less likely to contain unsupported features (data structures that Uproot can't read might be in a branch you don't need to read);\n",
    "   * often faster because they can be read in a single NumPy call, rather than many Python statements;\n",
    "   * possible to read one column at a time, without touching the others (in ROOT and Uproot).\n",
    "\n",
    "Let's look at an example of the same data in unsplit and split form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsplit = uproot.open(\"data/small-evnt-tree-nosplit.root\")[\"tree\"]\n",
    "split = uproot.open(\"data/small-evnt-tree-fullsplit.root\")[\"tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsplit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the unsplit data, and they are Python objects with attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsplit_events = unsplit[\"evt\"].array()\n",
    "unsplit_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsplit_events[5]._SliceI64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could ask for all attributes of one event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{name: getattr(unsplit_events[5], \"_\" + name) for name in unsplit_events[5]._fields}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could ask for the same attribute from all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_numpy_arrays = [x._SliceI64 for x in unsplit_events]\n",
    "list_of_numpy_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approximates what the split file naturally has: a column representing a single field of all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jagged_array = split[\"SliceI64\"].array()\n",
    "jagged_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks different because it is different:\n",
    "\n",
    "   * the Python list comprehension over unsplit objects made a list of NumPy arrays;\n",
    "   * the split data was directly read into a JaggedArray.\n",
    "\n",
    "The JaggedArray has features that the list of NumPy arrays doesn't (more on Awkward Array in the second hour).\n",
    "\n",
    "For instance, you can slice the second dimension, which has variable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jagged_array[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not in a Python list of NumPy arrays. Python doesn't think of the objects in the list as being part of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "list_of_numpy_arrays[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through a construction, we can build the same kind of objects from unsplit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward1 as ak\n",
    "\n",
    "events = ak.Array([{name: getattr(obj, \"_\" + name) for name in obj._fields if name != \"P3\"} for obj in unsplit_events])\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now an array of everything; its type shows the full structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.type(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.SliceI64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.from_awkward0(jagged_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.from_awkward0(jagged_array) == events.SliceI64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.all(ak.from_awkward0(jagged_array) == events.SliceI64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you shouldn't have to write this manually. Uproot 4 will do that for you (taking advantage of some Awkward 1 features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "Sometimes, though, we want objects to have methods. TTree (also auto-generated from `TStreamerInfo`, like anything else) is a prime example: we want TTrees to have methods that read TBaskets and convert them into arrays.\n",
    "\n",
    "Uproot has a stable set of \"mixin classes,\" which define methods but no data, as well as the auto-generated \"models\" that deserialize and store data. Runtime classes inherit from both.\n",
    "\n",
    "Histograms, for instance, have some analysis methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = uproot.open(\"data/hepdata-example.root\")\n",
    "histograms.classnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpx\"].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shout-out: see [scikit-hep/histoprint](https://github.com/ast0815/histoprint) for a more fully featured package that will take over the job of pretty-printing histograms.\n",
    "\n",
    "It can do overlays, stacks, and terminal colors (not in Jupyter, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import histoprint\n",
    "\n",
    "histoprint.print_hist(histograms[\"hpx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram methods are convenient ways to access C++ private members. For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpx\"]._fTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpx\"].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpx\"]._fXaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpx\"].bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpx\"].edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(histograms[\"hpx\"].hepdata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numpy` method turns the ROOT histogram into the same form that `np.histogram` would return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpx\"].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for 2-D histograms and `np.histogram2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms[\"hpxpy\"].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Matplotlib, the predominant Python plotting package, does not like to take prebinned histogram data.\n",
    "\n",
    "This idea of filling histograms in a separate job from plotting them, which [we've been doing at least since HBOOK was released in 1974](https://indico.cern.ch/event/667648/attachments/1526850/2425425/cern17.pdf), is largely unknown beyond particle physics.\n",
    "\n",
    "The best you can do is a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "content, edges = histograms[\"hpx\"].numpy()\n",
    "\n",
    "plt.bar(edges[:-1], content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are projects in Scikit-HEP that are seeking to address that (another shout-out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplhep as hep\n",
    "\n",
    "hep.histplot(histograms[\"hpx\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "hep.histplot(histograms[\"hpx\"].numpy(), ax=ax1)\n",
    "ax1.set_xlabel(histograms[\"hpx\"].title)\n",
    "\n",
    "content, ((xbins, ybins),) = histograms[\"hpxpy\"].numpy()\n",
    "hep.hist2dplot(content, xbins, ybins, ax=ax2)\n",
    "ax2.set_xlabel(histograms[\"hpxpy\"].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For histogramming, take a look at the following. With the exception of `hist` (a new project in Scikit-HEP), they are all complete, stable, and actively maintained.\n",
    "\n",
    "   * [Boost.Histogram](https://www.boost.org/doc/libs/1_73_0/libs/histogram/doc/html/index.html): minimal-dependencies, fast-filling, flexible, HEP-style histograms in C++, which has been accepted into Boost.\n",
    "   * [boost-histogram](https://github.com/scikit-hep/boost-histogram): Python bindings for Boost.Histogram.\n",
    "   * [mplhep](https://github.com/scikit-hep/mplhep): plotting interface over Matplotlib, providing CMS and ATLAS styles and other HEP conveniences.\n",
    "   * [histoprint](https://github.com/ast0815/histoprint): histogram renderer for terminals and command-lines.\n",
    "   * [Physt](https://github.com/janpipek/physt): complete histogramming package with a HEP-like point of view.\n",
    "   * [hist](https://github.com/scikit-hep/hist): Pythonic \"one-stop-shop\" for histogramming, pulling in all dependencies to make plotting easier. Filling via boost-histogram, plotting via mplhep, text output via histoprint.\n",
    "   * [Coffea histograms](https://coffeateam.github.io/coffea/notebooks/histograms.html): intended as an intermediate, presages some of the features of hist.\n",
    "\n",
    "Of course there's also [PyROOT](https://root.cern.ch/pyroot) and [rootpy](https://pypi.org/project/rootpy). If you're a theorist, you've probably heard of [YODA](https://yoda.hepforge.org/pydoc).\n",
    "\n",
    "But there's many others, too: [fast-histogram](https://pypi.org/project/fast-histogram), [qhist](https://pypi.org/project/qhist), [hdrhistogram](https://pypi.python.org/pypi/hdrhistogram), [multihist](https://pypi.python.org/pypi/multihist), [matplotlib-hep](https://github.com/ibab/matplotlib-hep), [pyhistogram](https://pypi.python.org/pypi/pyhistogram), [histogram](https://pypi.python.org/pypi/histogram), [SimpleHist](https://pypi.python.org/pypi/SimpleHist), [paida](https://pypi.org/project/paida), [histogramy](https://pypi.python.org/pypi/histogramy), [pypeaks](https://pypi.python.org/pypi/pypeaks), [hierogram](https://pypi.python.org/pypi/hierogram), [histo](https://pypi.python.org/pypi/histo), [python-metrics](https://pypi.python.org/pypi/python-metrics), [statscounter](https://pypi.python.org/pypi/statscounter), [datagram](https://pypi.python.org/pypi/datagram), [histogram](https://github.com/theodoregoetz/histogram) and [dashi](http://www.ifh.de/~middell/dashi/index.html), most of which seem to have been abandoned.\n",
    "\n",
    "That's not even counting the six I've written: [plothon](http://code.google.com/p/plothon), [svgfig](http://code.google.com/p/svgfig), [cassius](https://github.com/opendatagroup/cassius), [histogrammar](https://github.com/histogrammar), [histbook](https://github.com/scikit-hep/histbook), and [aghast](https://github.com/scikit-hep/aghast) (though this last one is a middleware tool, not user-facing).\n",
    "\n",
    "**Moral:** starting a histogram package is easy, growing a community around one so that it develops is hard.\n",
    "\n",
    "Uproot's histogram functionality will defer more to [hist](https://github.com/scikit-hep/hist) as it develops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active objects from TTrees\n",
    "\n",
    "Above, you've seen how we could extract auto-generated `Event` objects from a TTree, as well as auto-generated histograms from a TDirectory.\n",
    "\n",
    "Can objects from a TTree have methods?\n",
    "\n",
    "**Yes!** In fact, they can be histograms. (Thanks to Cédric Hernalsteens for supplying this example in [Uproot issue #399](https://github.com/scikit-hep/uproot/issues/399).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_histograms = uproot.open(\"data/issue399.root\")[\"Event/Histos.histograms1D\"].array()\n",
    "array_of_histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In fact, the issue was that these are _lists_ of histograms in each TTree entry.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 6, figsize=(18, 30))\n",
    "fig.subplots_adjust(bottom=-0.1, left=-0.2)\n",
    "\n",
    "i, j = 0, 0\n",
    "for hists in array_of_histograms:\n",
    "    for hist in hists:\n",
    "        hep.histplot(hist.numpy(), ax=axes[i][j])\n",
    "        axes[i][j].set_xlabel(hist.title)\n",
    "        j += 1\n",
    "        if j == 6:\n",
    "            i += 1\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorentz vectors\n",
    "\n",
    "Perhaps the most important active objects are Lorentz vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lorentz = uproot.open(\"data/HZZ.root\")[\"events\"]\n",
    "with_lorentz = uproot.open(\"data/HZZ-objects.root\")[\"events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lorentz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_lorentz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that this is a jagged array of objects, Lorentz vectors have a fixed-width structure and can be extracted in a fast NumPy call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_array = with_lorentz[\"muonp4\"].array()\n",
    "lorentz_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another consequence is that the mix-in methods (e.g. `pt`, `eta`, `phi`) can be attached to the JaggedArray as well as the individual objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_array[32, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_array[32, 3].pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_array[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_array[32].pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorentz_array.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of \"lifting\" an operation from scalar → scalar, like Lorentz object → pT, to array → array and even jagged array → jagged array is the heart of columnar analysis.\n",
    "\n",
    "We'll be seeing more of it in the session on Awkward Array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, this is how we can do a Z mass peak in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((lorentz_array[lorentz_array.counts >= 2, 0] + lorentz_array[lorentz_array.counts >= 2, 1]).mass, bins=100, range=(60, 120));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing ROOT files\n",
    "\n",
    "Reading and writing are asymmetric: they come with different sets of issues.\n",
    "\n",
    "Uproot was originally intended for reading and it has more reading functionality, but it can do quite a bit of writing, now, too. (Thanks to Pratyush Das!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = uproot.recreate(\"tmp.root\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interface still works like a Python dict: you add objects to the ROOT file by assigning them.\n",
    "\n",
    "(The name goes in the square brackets after the file, not in the object.)\n",
    "\n",
    "Pythonic types, such as a NumPy histogram, are accepted and converted into ROOT histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "output_file[\"histogram\"] = np.histogram(np.random.normal(0, 1, 1000000), bins=100, range=(-3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are really reading the object back and looking at the C++ member data that was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"histogram\"].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To come full-circle, we can convert the ROOT histogram into NumPy form and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot(output_file[\"histogram\"].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing TTrees\n",
    "\n",
    "TTrees have a special interface because you'll likely need to write the data incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree\"] = uproot.newtree({\"branch1\": int, \"branch2\": float, \"branch3\": np.int32})\n",
    "output_file[\"tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree\"].extend({\"branch1\": np.random.poisson(3, 10000),\n",
    "                            \"branch2\": np.random.normal(0, 1, 10000),\n",
    "                            \"branch3\": np.random.poisson(1.2, 10000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree\"].numentries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree\"].extend({\"branch1\": np.random.poisson(3, 10000),\n",
    "                            \"branch2\": np.random.normal(0, 1, 10000),\n",
    "                            \"branch3\": np.random.poisson(1.2, 10000)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree\"].numentries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity is pay-as-you-go. You can add titles to the branches, though you'll need the `uproot.newbranch` function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree2\"] = uproot.newtree({\"branch1\": uproot.newbranch(float, title=\"snazzy branch\")}, title=\"snazzy tree\")\n",
    "output_file[\"tree2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree2\"].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"tree2/branch1\"].title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing JaggedArrays to TTrees\n",
    "\n",
    "This is a very new feature, but you can do it. You just have to set another branch as its `size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jagged_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jagged_array.counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a `size` creates that branch (there's only one type it can have: int32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"jagged_tree\"] = uproot.newtree({\"branch1\": uproot.newbranch(np.dtype(\">f4\"), size=\"n\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file[\"jagged_tree\"].extend({\"branch1\": jagged_array, \"n\": jagged_array.counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uproot.open(\"tmp.root\")[\"jagged_tree/branch1\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading many arrays at once\n",
    "\n",
    "So far, we've only been using the TBranch.array method to get arrays, but TTree.arrays (plural) is a convenient way to get a whole pack of them.\n",
    "\n",
    "(In Uproot 4, TTree.arrays can also be more efficient via XRootD vector-reads and HTTP multipart-GETs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = uproot.open(\"data/nesteddirs.root\")[\"one/two/tree\"]\n",
    "tree.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a list of branch names to get a dict of names → arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.arrays([\"Int32\", \"Int64\", \"Str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `outputtype=tuple` to get a tuple..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.arrays([\"Int32\", \"Int64\", \"Str\"], outputtype=tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which is good for unpacking (it preserves order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Int32, Int64, Str = tree.arrays([\"Int32\", \"Int64\", \"Str\"], outputtype=tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output type can also be a Pandas DataFrame, though the alternate syntax TTree.pandas.df is more often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tree.arrays([\"Int32\", \"Int64\", \"Str\"], outputtype=pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use wildcards (same syntax as in a UNIX shell) to match all by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.arrays(\"Slice*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or surround the string with `/` for a regular expression search (same syntax as Python's `re` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.arrays(r\"/Slice[UF].*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology preview: all of the above in Uproot 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_uproot4 = uproot4.open(\"data/nesteddirs.root\")[\"one/two/tree\"]\n",
    "tree_uproot4.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `library=\"np\"`, you get a dict of NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_uproot4.arrays([\"Int32\", \"Int64\", \"Float32\"], library=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `library` is the default `\"ak\"` (Awkward Arrays), you can get a dict of Awkward Arrays with `how=dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_uproot4.arrays([\"Int32\", \"Int64\", \"Float32\"], how=dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the _default_ way to get a group of arrays is zipped into an Awkward record array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_uproot4.arrays([\"Int32\", \"Int64\", \"Float32\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an array of records with field names `Int32`, `Int64`, `Float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.type(tree_uproot4.arrays([\"Int32\", \"Int64\", \"Float32\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the way that old Uproot combined arrays into a single object when returning a Pandas DataFrame, but now `library` and `how` are decoupled.\n",
    "\n",
    "   * NumPy's natural grouping is dict, but tuple and list are also allowed.\n",
    "   * Pandas's natural grouping (of Series) is DataFrame, but dict, tuple, and list are allowed.\n",
    "   * Awkward's natural grouping is a record array, but dict, tuple, and list are allowed.\n",
    "\n",
    "In Pandas, `how` also specifies how data with different jaggedness, such as the muons and jets in events, are merged. There isn't a one-to-one relationship between each muon and each jet, but there are JOIN techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single array is a Pandas Series..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lorentz_uproot4 = uproot4.open(\"data/HZZ.root | events\")\n",
    "\n",
    "without_lorentz_uproot4[\"Muon_Px\"].array(library=\"pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A group of arrays is a DataFrame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lorentz_uproot4.arrays([\"Muon_Px\", \"Muon_Py\", \"Muon_Pz\"], library=\"pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if they have different jaggedness (e.g. muons and jets), by default you get multiple DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lorentz_uproot4.arrays([\"Muon_Px\", \"Jet_Px\", \"Muon_Py\", \"Jet_Py\"], library=\"pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `how` can be passed to Pandas to define some merging semantics: INNER JOIN, LEFT JOIN, RIGHT JOIN, OUTER JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lorentz_uproot4.arrays([\"Muon_Px\", \"Muon_Py\", \"Jet_Px\", \"Jet_Py\"], library=\"pd\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lorentz_uproot4.arrays([\"Muon_Px\", \"Muon_Py\", \"Jet_Px\", \"Jet_Py\"], library=\"pd\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, Awkward's default is to combine arrays in a shallow way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.type(without_lorentz_uproot4.arrays([\"Muon_Px\", \"Muon_Py\", \"Jet_Px\", \"Jet_Py\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if `how=\"zip\"`, then it will zip together arrays with the same jaggedness (and not arrays with different jaggedness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.type(without_lorentz_uproot4.arrays([\"Muon_Px\", \"Muon_Py\", \"Jet_Px\", \"Jet_Py\"], how=\"zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.to_list(without_lorentz_uproot4.arrays([\"Muon_Px\", \"Muon_Py\", \"Jet_Px\", \"Jet_Py\"], how=\"zip\")[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name splicing was intended for NanoAOD, though it could use a little work in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms_uproot4 = uproot4.open(\"data/cms_opendata_2012_nanoaod_DoubleMuParked.root | Events\")\n",
    "cms_uproot4.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.to_list(cms_uproot4.arrays([\"Muon_pt\", \"Muon_eta\", \"Muon_phi\"], how=\"zip\", entry_stop=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings passed to TTree.arrays can be mathematical expressions, and they can be indirect (through aliases).\n",
    "\n",
    "This is to support TTree aliases, which caught me by suprise in old Uproot (I hadn't used them before—I didn't realize they could be expressions).\n",
    "\n",
    "There's no performance advantage in computing with strings vs computing with Python commands (and the syntax in the strings is Python, for now); this is to support a widely used convenience.\n",
    "\n",
    "(A change that undermines the assumption that these strings are TBranch names had to wait for a major revision like this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cms_uproot4.arrays(\"PV_xy\", aliases={\"PV_xy\": \"sqrt(PV_x**2 + PV_y**2)\"}, functions={\"sqrt\": np.sqrt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Lorentz vector package\n",
    "\n",
    "Since Lorentz vectors have turned out to be the most important \"object with methods\" so far, they're getting proper treatment in a standalone library called [Vector](https://vector.readthedocs.io/en/latest/).\n",
    "\n",
    "It's in early stages of development, but Awkward 1 is being developed to support it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory management\n",
    "\n",
    "One of the first questions that was asked when I introduced Uproot as a package that \"loads whole TBranches as arrays\" was \"won't you run out of memory?\"\n",
    "\n",
    "In practice, Coffea analyses use about 10% of the columns of 2 GB files, so 0.2 GB for the initial arrays × all the derived quantities still fits within the RAM available on most machines. Analyses that process one file at a time are generally okay loading whole TBranches.\n",
    "\n",
    "However, you'll probably run into a limit at _some_ point, so it's good to know ways around it.\n",
    "\n",
    "First, note that arrays can be partially read if you use `entrystart` and `entrystop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = uproot.open(\"data/cms_opendata_2012_nanoaod_DoubleMuParked.root\")[\"Events\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:** reads the whole TBranch and only _views_ the first 10 events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms[\"event\"].array()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:** only reads the TBaskets necessary to get the first 10 events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms[\"event\"].array(entrystart=0, entrystop=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since the first event has 243206 entries in it, setting `entrystop=10` means reading 243206 and viewing 10.\n",
    "\n",
    "Reading less than one TBasket is not possible (because compressed chunks need to be fully decompressed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cms[\"event\"].basket_numentries(i) for i in range(cms[\"event\"].numbaskets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, reading the whole thing and slicing after the fact means reading 1000000 and viewing 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.numentries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's an optimal size for TBaskets: small enough to easily fit into memory and large enough to spend more time in NumPy number-crunching than Python book-keeping.\n",
    "\n",
    "Usually, TBaskets are _too small_. (I would guess that they're tuned for the original NanoAOD file size and TBaskets are not merged when the entries are heavily filtered.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** a lot of these parameter names will be be \"camel_case\" in Uproot 4: `entry_start`, `entry_stop`, `num_entries`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating through a file\n",
    "\n",
    "If you're working on a set of TBranches that are too large to fit into memory, you'll probably want to slice it iteratively, such that the `entrystop` of the last batch is the `entrystart` of the next batch.\n",
    "\n",
    "TTree.iterate does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in cms.iterate(\"Muon_*\"):\n",
    "    print({name: len(array) for name, array in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, it slices at entry numbers where the TBasket boundaries all line up for the set of TBranches you're looking at.\n",
    "\n",
    "That way, you get consistent arrays (they're all the same length and represent the same events) and avoid the \"slop\" of incomplete TBaskets.\n",
    "\n",
    "You can do a \"dry run\" to see where these entry boundaries would be using TTree.clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cms.clusters(\"Muon_*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can depend on which TBranches you're looking at because one TBranch with oddly spaced TBasket boundaries can ruin the alignment of the rest.\n",
    "\n",
    "Here, we look at just the four kinematic variables, and they're more fine-grained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cms.clusters([\"Muon_pt\", \"Muon_eta\", \"Muon_phi\", \"Muon_mass\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which was the offending TBranch? To start with, let's look at the set that we're considering..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.keys(filtername=lambda name: name.startswith(b\"Muon_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and remove TBaskets until we find the ones that changed the spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cms.clusters(lambda branch: branch.name.startswith(b\"Muon_\") and branch.name not in (b\"Muon_pfRelIso04_all\", b\"Muon_tightId\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But maybe you don't care about that—just pick a fixed number of entries for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in cms.iterate(\"Muon_*\", entrysteps=100000):\n",
    "    print({name: len(array) for name, array in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBaskets that were only partially used in one step in the iteration are saved for the next step, so they're not re-read/decompressed.\n",
    "\n",
    "We can see this by the fact that the time-to-read/decompress is not much different between the two cases: partial TBaskets do not cause data to be re-read/decompressed, since that would be very costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for batch in cms.iterate(\"Muon_*\"):\n",
    "    {name: len(array) for name, array in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "for batch in cms.iterate(\"Muon_*\", entrysteps=100000):\n",
    "    {name: len(array) for name, array in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more complication: \"number of entries\" is not a great measure of \"amount of data.\"\n",
    "\n",
    "Some data types use more bytes than others, but there's also the fact that you might quickly switch from needing two TBranches to needing ten TBranches. You don't want to re-tune your number of entries.\n",
    "\n",
    "Considering that this was motivated by wanting to fit everything in memory, you can scale the size of your batches by a number of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in cms.iterate(\"Muon_*\", entrysteps=\"10 MB\"):\n",
    "    print({name: len(array) for name, array in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, if you use more or less (or different) TBaskets, you're still using _about_ the same memory in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in cms.iterate(lambda branch: not branch.name.startswith(b\"Muon_\"), entrysteps=\"10 MB\"):\n",
    "    print({name: len(array) for name, array in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in cms.iterate([\"Muon_pfRelIso04_all\", \"Muon_tightId\"], entrysteps=\"10 MB\"):\n",
    "    print({name: len(array) for name, array in batch.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like TTree.clusters, you can get the entry boundaries as a dry run with TTree.mempartitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cms.mempartitions(\"10 MB\", [\"Muon_pt\", \"Muon_eta\", \"Muon_phi\", \"Muon_mass\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing many files (TChain)\n",
    "\n",
    "ROOT's TChain makes the interface for iterating over _entries_ in many TTrees the same as iterating over _entries_ in only one TTree.\n",
    "\n",
    "Uproot either gives you all entries in a TTree (the `arrays` method) or batches of entries in a TTree (the `iterate` method).\n",
    "\n",
    "   * Extending `arrays` to many files would run you out of memory fast: imagine an interface that loaded all requested TBranches of a large set of files and concatenated them. You can concatenate arrays manually ([np.concatenate](https://numpy.org/doc/1.18/reference/generated/numpy.concatenate.html) or [ak.concatenate](https://awkward-array.readthedocs.io/en/latest/_auto/ak.concatenate.html)), but it's probably not what you want.\n",
    "   * Maybe it would be useful to have arrays that look and act like \"all the files\" but only read on demand: these would be `lazyarrays`.\n",
    "   * Extending `itetate` to many files makes sense: you can use the same interface that would step over batches within a file when accessing many files.\n",
    "\n",
    "Uproot 3 has all of these interfaces:\n",
    "\n",
    "<table width=\"100%\" style=\"font-size: 1.25em\"><tr style=\"background: white;\">\n",
    "    <td width=\"33%\" style=\"vertical-align: top\">\n",
    "        <p style=\"font-weight: bold; font-size: 1.5em; margin-bottom: 0.5em\">Direct</p>\n",
    "        <p>Read the file and return an array.</p>\n",
    "        <ul>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/ttree-handling.html#id11\">TBranch.array</a></li>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/ttree-handling.html#array\">TTree.array</a></li>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/ttree-handling.html#arrays\">TTree.arrays</a></li>\n",
    "        </ul>\n",
    "    </td><td width=\"33%\" style=\"vertical-align: top\">\n",
    "        <p style=\"font-weight: bold; font-size: 1.5em; margin-bottom: 0.5em\">Lazy</p>\n",
    "        <p>Get an object that reads on demand.</p>\n",
    "        <ul>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/ttree-handling.html#id13\">TBranch.lazyarray</a></li>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/ttree-handling.html#lazyarray\">TTree.lazyarray</a></li>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/ttree-handling.html#lazyarrays\">TTree.lazyarrays</a></li>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/opening-files.html#uproot-lazyarray-and-lazyarrays\">uproot.lazyarray</a>*</li>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/opening-files.html#uproot-lazyarray-and-lazyarrays\">uproot.lazyarrays</a>*</li>\n",
    "        </ul>\n",
    "    </td><td width=\"33%\" style=\"vertical-align: top\">\n",
    "        <p style=\"font-weight: bold; font-size: 1.5em; margin-bottom: 0.5em\">Iterative</p>\n",
    "        <p>Read arrays in batches of entries.</p>\n",
    "        <ul>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/ttree-handling.html#iterate\">TTree.iterate</a></li>\n",
    "            <li style=\"margin-bottom: 0.3em\"><a href=\"https://uproot.readthedocs.io/en/latest/opening-files.html#uproot-iterate\">uproot.iterate</a>*</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "<p>* Applies to sets of files, like TChain.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But be warned:** lazy arrays rely on features of Awkward 0 that were hard to get right and still have outstanding bugs.\n",
    "\n",
    "Awkward 1 has a better (more airtight) implementation of lazy arrays, so this will probably be better in Uproot 4.\n",
    "\n",
    "However, laziness is not always helpful. Your array \"loads\" right away, but maybe you always pay a performance penalty in calculations because\n",
    "\n",
    "```python\n",
    "px**2 + py**2\n",
    "```\n",
    "\n",
    "walks over _all files_ to compute `px**2`, then walks over _all files_ to compute `py**2`, and maybe doesn't have enough memory to add `px**2` to `py**2`.\n",
    "\n",
    "At some level, we need to define a batch size and do all computations in that batch, including derived quantities, before moving on to the next. The `uproot.iterate` interface naturally does that, fencing your calculations within a `for` block, but maybe we can define something similar with lazy arrays and Dask.\n",
    "\n",
    "**Or maybe if you're in this situation, you should move on from bare Uproot and look into the scale-out mechanisms Coffea, IRIS-HEP, and others are developing.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
